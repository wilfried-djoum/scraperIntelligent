# ScraperIntelligent

API de profiling professionnel intelligent combinant web scraping et analyse LLM.

### 1. D√©mo Fonctionnelle

**Ex√©cution en local:**
```bash
# Cloner le repository
git clone <repo-url>
cd ScraperIntelligent

# Installer les d√©pendances
pip install -r requirements.txt

# Configurer les variables d'environnement
# Cr√©er un fichier .env √† la racine avec:
# FIRECRAWL_API_KEY=votre_cl√©
# OPENAI_API_KEY=votre_cl√©

# Lancer l'application
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Acc√©der √† l'interface web
# http://localhost:8000/static/index.html
```

**D√©ploiement (Vercel/Heroku/Railway):**
- Voir [DEPLOYMENT.md](DEPLOYMENT.md) pour les instructions d√©taill√©es
- Configuration des variables d'environnement requise
- Compatible avec Docker (Dockerfile inclus)

### 2. Code Propre et Structur√©

**Qualit√© du code:**
- ‚úì Architecture modulaire (Orchestrator pattern)
- ‚úì S√©paration des responsabilit√©s (services, models, config)
- ‚úì Main.py r√©duit de 82% (340 ‚Üí 59 lignes)
- ‚úì Commentaires docstrings sur toutes les fonctions publiques
- ‚úì Type hints Python (Pydantic models)
- ‚úì Configuration centralis√©e (.env + config.py)
- ‚úì Gestion d'erreurs coh√©rente

**Standards suivis:**
- PEP 8 (formatting Python)
- Single Responsibility Principle
- DRY (Don't Repeat Yourself) via BaseScraper
- Dependency Injection (config singleton)

### 3. Documentation Technique

**Documents fournis:**
- [README.md](README.md) - Vue d'ensemble et guide de d√©marrage rapide
- [ARCHITECTURE.md](ARCHITECTURE.md) - Architecture d√©taill√©e et diagrammes
- Commentaires inline dans le code source

**Couverture:**
- ‚úì Architecture syst√®me (patterns, flux de donn√©es)
- ‚úì Choix techniques justifi√©s (Firecrawl, OpenAI, FastAPI)
- ‚úì Limites actuelles document√©es
- ‚úì Pistes d'am√©lioration prioris√©es

---

## Architecture Refactoris√©e

### Structure du Projet

```
ScraperIntelligent/
‚îú‚îÄ‚îÄ main.py                          # Point d'entr√©e FastAPI (60 lignes)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ profile.py               # Mod√®les Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_scraper.py          # Classe de base pour scrapers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ profile_orchestrator.py  # Orchestration du workflow
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_analyzer.py          # Service LLM OpenAI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scoring.py               # Calcul du score de fiabilit√©
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sources/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ linkedin.py          # Scraper LinkedIn
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ company.py           # Scraper sites entreprise
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ news.py              # Scraper presse
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ social.py            # Scraper r√©seaux sociaux
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îú‚îÄ‚îÄ index.html
‚îÇ       ‚îú‚îÄ‚îÄ main.js
‚îÇ       ‚îî‚îÄ‚îÄ styles.css
```

## Am√©liorations du Refactoring

### 1. Configuration Centralis√©e (`src/config.py`)
- Toutes les cl√©s API dans un seul endroit
- Variables d'environnement avec valeurs par d√©faut
- Configuration Firecrawl (version, timeout, wait_for)
- Configuration OpenAI (model, temperature, max_tokens)
- Param√®tres de scraping (retries, max posts, etc.)

### 2. Classe de Base `BaseScraper` (`src/services/base_scraper.py`)
- Factorisation de l'initialisation Firecrawl
- Gestion SSL/CA bundle centralis√©e
- M√©thodes helper pour extraction s√ªre (markdown, html, metadata)
- M√©thode `_scrape_url()` g√©n√©rique
- Logs coh√©rents

### 3. Orchestrateur `ProfileOrchestrator` (`src/services/profile_orchestrator.py`)
- Extraction de toute la logique m√©tier hors de main.py
- Workflow d√©coup√© en m√©thodes priv√©es claires:
  - `_scrape_all_sources()` - Lance les scrapers
  - `_extract_profile_data()` - Extrait et structure les donn√©es
  - `_calculate_reliability()` - Calcule le score
  - `_assemble_profile()` - Assemble le profil final
- S√©paration des pr√©occupations (scraping / extraction / enrichissement / scoring)
- Testabilit√© am√©lior√©e

### 4. Main.py Simplifi√©
- R√©duit de ~340 lignes √† 60 lignes (-82%)
- Responsabilit√© unique: routing FastAPI
- D√©l√©gation compl√®te √† l'orchestrateur
- Documentation API am√©lior√©e

## Installation

```bash
# Cloner le repo
cd ScraperIntelligent

# Installer les d√©pendances
pip install -r requirements.txt

# Configurer les variables d'environnement (optionnel)
cp .env.example .env
# √âditer .env avec vos cl√©s API
```

## Configuration

### Variables d'Environnement

Cr√©er un fichier `.env` √† la racine:

```env
# API Keys
FIRECRAWL_API_KEY=your_firecrawl_key
OPENAI_API_KEY=your_openai_key

# Firecrawl Settings
FIRECRAWL_TIMEOUT=30
FIRECRAWL_WAIT_FOR=3000

# OpenAI Settings
OPENAI_MODEL=gpt-4o-mini
```

Les cl√©s sont √©galement hardcod√©es dans `src/config.py` pour le d√©veloppement (√† retirer en production).

## Lancement

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

API accessible sur:
- Interface Web: http://localhost:8000/static/index.html
- API Docs: http://localhost:8000/docs
- Endpoint profiling: POST http://localhost:8000/profiling/

## Usage de l'API

### Endpoint `/profiling/`

**Request:**
```json
POST /profiling/
{
    "first_name": "Satya",
    "last_name": "Nadella",
    "company": "Microsoft"
}
```

**Response:**
```json
{
    "debug": {
        "sources_used": ["linkedin", "company", "news", "social"],
        "processing_time": "34.2s"
    },
    "profile": {
        "first_name": "Satya",
        "last_name": "Nadella",
        "company": "Microsoft",
        "headline": "CEO at Microsoft",
        "summary": "...",
        "current_role": "Chief Executive Officer",
        "experiences": [...],
        "skills": [...],
        "education": [...],
        "publications": [...],
        "linkedin_analysis": {...},
        "contact_info": {...},
        "reliability": {
            "score": 85,
            "justification": "...",
            "factors": [...]
        },
        "reputation": {...},
        "sources_used": [...]
    }
}
```

## üîß Choix Techniques

### Framework Backend: FastAPI
**Pourquoi FastAPI?**
- Performance √©lev√©e (bas√© sur Starlette + Pydantic)
- Documentation API automatique (Swagger UI)
- Type safety native avec Python type hints
- Async/await pour I/O non-bloquant
- Facilit√© de d√©ploiement

### Scraping: Firecrawl v2
**Pourquoi Firecrawl?**
- API manag√©e (pas de maintenance de navigateurs headless)
- Support JavaScript rendering
- Rate limiting g√©r√© c√¥t√© serveur
- Extraction markdown/html structur√©e
- Alternative √† Selenium/Playwright plus simple

### LLM: OpenAI gpt-4o-mini
**Pourquoi gpt-4o-mini?**
- Co√ªt optimis√© (5-10x moins cher que GPT-4)
- Latence r√©duite (~2-3s par requ√™te)
- Capacit√©s de structuration suffisantes
- Fallback sur knowledge base (Oct 2023)
- Alternative: Claude-3.5-Sonnet, Mistral, Llama-3

### Architecture: Orchestrator Pattern
**Pourquoi ce pattern?**
- S√©paration claire des responsabilit√©s
- Workflow complexe coordonn√© (4 scrapers ‚Üí extraction ‚Üí LLM ‚Üí scoring)
- Testabilit√© (chaque service isol√©)
- √âvolutivit√© (ajout de nouvelles sources facile)

### Frontend: Vanilla JS
**Pourquoi pas React/Vue?**
- Pas de build step n√©cessaire
- D√©ploiement statique simple
- Overhead minimal pour une SPA simple
- Chargement instantan√©

## Points Techniques Importants

### Firecrawl v2
Tous les scrapers utilisent maintenant Firecrawl v2:
```python
self.firecrawl = FirecrawlApp(api_key=config.FIRECRAWL_API_KEY, version="v2")
result = self.firecrawl.scrape_url(url, formats=["markdown"], onlyMainContent=True)
markdown = getattr(result, 'markdown', '')  # Acc√®s via attributs, pas dict
```

### OpenAI LLM
- Mod√®le: **gpt-4o-mini** (coupure octobre 2023)
- Utilisations:
  1. `clean_and_structure()` - Extraction structur√©e du contenu scrap√©
  2. `summarize_posts()` - Analyse des posts LinkedIn
  3. `global_synthesis()` - Synth√®se narrative du profil
  4. `justify_reliability()` - Justification du score
  5. `enrich_from_knowledge()` - **Fallback** si scraping √©choue (donn√©es 2023 uniquement)

### Score de Fiabilit√©
- Base 0-100
- **Sources** (max 40 pts): +10 par source (LinkedIn/Company/News/Social)
- **Compl√©tude** (max 60 pts): headline(8), summary(10), experiences(12), publications(8), posts(8), education(6), skills(4), social(4)
- P√©nalit√© si donn√©es LLM avec faible confiance

## Limites Actuelles

### 1. Scraping LinkedIn
**Probl√®me:** 403 Forbidden pour la plupart des profils publics  
**Cause:** Anti-scraping agressif de LinkedIn (Cloudflare, CAPTCHA)  
**Impact:** Donn√©es LinkedIn limit√©es ou absentes  
**Workaround:** Enrichissement LLM fallback (donn√©es Oct 2023)

### 2. Connaissance LLM Dat√©e
**Probl√®me:** gpt-4o-mini coupure octobre 2023  
**Cause:** Limitation intrins√®que du mod√®le  
**Impact:** Informations r√©centes (2024-2025) non disponibles via fallback  
**Workaround:** Scraping web reste la source primaire

### 3. Fragilit√© du Scraping HTML
**Probl√®me:** Sites peuvent changer leur structure  
**Cause:** Pas d'API officielle, parsing HTML  
**Impact:** Scrapers peuvent casser sans pr√©avis  
**Workaround:** Logs d√©taill√©s + monitoring + retry logic

### 4. Rate Limiting
**Probl√®me:** Firecrawl et OpenAI ont des quotas  
**Cause:** Plans API limit√©s  
**Impact:** Erreurs 429 en production haute charge  
**Workaround:** Cache Redis + backoff exponentiel (roadmap)

### 5. Performance
**Probl√®me:** Temps de r√©ponse 30-45s par profil  
**Cause:** 4 scrapers s√©quentiels + 5 appels LLM  
**Impact:** UX d√©grad√©e pour l'utilisateur  
**Workaround:** Loader anim√© + parall√©lisation (roadmap)

## Pistes d'Am√©lioration

### Court Terme (1-2 semaines)
**Priorit√© HAUTE:**
- [ ] **Parall√©lisation des scrapers** ‚Üí R√©duire temps √† ~15-20s
  - Utiliser `asyncio.gather()` pour scrapers ind√©pendants
  - Gains: 50% temps de r√©ponse

- [ ] **Cache Redis** ‚Üí √âviter re-scraping
  - TTL 24h pour profils
  - Gains: 90% r√©duction co√ªts API

- [ ] **Tests unitaires** ‚Üí Garantir stabilit√©
  - Coverage 80%+ sur services
  - CI/CD avec GitHub Actions

**Priorit√© MOYENNE:**
- [ ] **Retry automatique** ‚Üí Resilience
  - Backoff exponentiel (1s, 2s, 4s)
  - Circuit breaker pattern

- [ ] **Logging structur√©** ‚Üí Debuggabilit√©
  - JSON logs avec contexte
  - Agr√©gation Datadog/Sentry

### Moyen Terme (1-2 mois)
**Fonctionnalit√©s:**
- [ ] **API de recherche temps r√©el** ‚Üí Donn√©es fra√Æches
  - Int√©gration Tavily/Perplexity
  - Donn√©es post-Oct 2023

- [ ] **Webhooks** ‚Üí Async profiling
  - Callback URL fournie par client
  - Profiling en arri√®re-plan

- [ ] **Multi-profils batch** ‚Üí Scalabilit√©
  - Upload CSV ‚Üí API traite liste
  - Rate limiting intelligent

**Infrastructure:**
- [ ] **Monitoring** ‚Üí Observabilit√©
  - Prometheus + Grafana
  - Alerting Slack/PagerDuty

- [ ] **Rate limiting par IP** ‚Üí Protection
  - SlowAPI middleware
  - Quotas par tier (free/pro)

### Long Terme (3-6 mois)
**R&D:**
- [ ] **Fine-tuning LLM** ‚Üí Pr√©cision
  - Dataset propri√©taire de profils
  - Mod√®le sp√©cialis√© extraction

- [ ] **Graph database** ‚Üí Relations
  - Neo4j pour liens entre profils
  - Analyse r√©seau

- [ ] **Computer Vision** ‚Üí OCR certificats
  - Extraction dipl√¥mes/certificats PDF
  - Validation automatique

## Tests

### Test Manuel Rapide
```bash
# Lancer un test avec une personnalit√© publique connue
curl -X POST http://localhost:8000/profiling/ \
  -H "Content-Type: application/json" \
  -d '{"first_name": "Satya", "last_name": "Nadella", "company": "Microsoft"}'

# R√©sultat attendu: Score 70-90/100, donn√©es partielles (LinkedIn bloqu√©)
```

### Tests Unitaires (√Ä venir)
```bash
pip install pytest pytest-asyncio pytest-cov
pytest tests/ --cov=src --cov-report=html
```